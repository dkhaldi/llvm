sycl_ext_oneapi_matrix

:source-highlighter: coderay
:coderay-linenums-mode: table

// This section needs to be after the document title.
:doctype: book
:toc2:
:toc: left
:encoding: utf-8
:lang: en
:dpcpp: pass:[DPC++]

// Set the default source code type in this document to C++,
// for syntax highlighting purposes.  This is needed because
// docbook uses c++ and html5 uses cpp.
:language: {basebackend@docbook:c++:cpp}


== Notice

[%hardbreaks]
Copyright (c) 2021-2022 Intel Corporation.  All rights reserved.

Khronos(R) is a registered trademark and SYCL(TM) and SPIR(TM) are trademarks
of The Khronos Group Inc.  OpenCL(TM) is a trademark of Apple Inc. used by
permission by Khronos.

== Contact

To report problems with this extension, please open a new issue at:

https://github.com/intel/llvm/issues

== Dependencies

This extension is written against the SYCL 2020 revision 6 specification.  All
references below to the "core SYCL specification" or to section numbers in the
SYCL specification refer to that revision.

== Status
This is an experimental extension specification, intended to provide early
access to features and gather community feedback.  Interfaces defined in this
specification are implemented in {dpcpp}, but they are not finalized and may
change incompatibly in future versions of {dpcpp} without prior notice.
*Shipping software products should not rely on APIs defined in this
specification.*

== Backend support status
This extension is currently implemented in {dpcpp} only for devices
that contain a matrix hardware, specifically Intel(R) Advanced Matrix
Extensions (Intel(R) AMX), Intel(R) Xe Matrix Extensions (Intel(R)
XMX) and Nvidia(R) Tensor Cores.

The `joint_matrix_mad` function is an optional kernel feature as defined
in section 5.7 of the core SYCL specification.  Each device supports
only certain values for the `M`, `N`, and `K` template parameters and
only certain types for the `Ta`, `Tb`, and `Tc` template parameters.
Applications can use the query API in `tpu_params` or
`get_info<experimental::info::device::matrix>` to determine the set of
legal parameters for each device.  If the application submits a kernel using
an unsupported `joint_matrix_mad` combination, the implementation throws a
synchronous exception with the `errc::kernel_not_supported` error code
as described in section 5.7.

== Overview
Joint matrix is a SYCL extension for matrix hardware programming. It
unifies targets like Intel AMX in CPUs, Intel XMX in Intel GPUs and
Nvidia Tensor Cores. This provides portable but performant API for
users who want to build their own neural networks applications,
perform custom optimzations, or experiment with new operations in a
timely and performing manner.

== Specification

=== Feature test macro

This extension provides a feature-test macro as described in the core SYCL
specification. An implementation supporting this extension must predefine
the macro `SYCL_EXT_ONEAPI_MATRIX` to one of the values defined in the
table below. Applications can test for the existence of this macro to
determine if the implementation supports this feature, or applications
can test the macro's value to determine which of the extension's
features the implementation supports.

[%header,cols="1,5"]
|===
|Value
|Description

|1
|The APIs of this experimental extension are not versioned, so the
 feature-test macro always has this value.
|===

=== New `joint_matrix` class
We introduce a new class called `joint_matrix`. The user needs to
specify the group memory scope, the type of the elements, the shape,
the matrix use, and the memory layout of the matrix. This results in
the following description:

```c++
namespace sycl::ext::oneapi::experimental::matrix {
template <typename Group, typename T, use Use, size_t Rows, size_t Cols,
          layout Layout = layout::dynamic>
struct joint_matrix {
    joint_matrix() {}
};
}
```

IMPORTANT: Matrix layout defaulting to `layout::dynamic` applies only
to `joint_matrix` with `use::accumulator`

==== Use
The main operation performed by the matrix hardware is `D=C+A*B`. `Use`
argument specifies the usage of the matrix: matrix left (`A`), matrix
right (`B`) or accumulator (`C`) and `D`. This is required by backend
implementations to reason about the layout of the matrix in registers.

```c++
namespace sycl::ext::oneapi::experimental::matrix {
enum class use {
  a,
  b,
  accumulator
};
}
```

==== Shape
The shape of a `joint_matrix` refers to its number of rows `Rows` and
number of columns `Cols`.

==== Layout
This specifies the memory layout and it can be row major or column major.

```c++
namespace sycl::ext::oneapi::experimental::matrix {
enum class layout {
  row_major,
  col_major,
  dynamic
 };
}
```

==== Group Memory Scope
In this API, we use the terminology of `joint_matrix` instead of plain
`matrix` to emphasize that the matrix is shared among a group of work
items and is not private to each work item. The group scope is added
as an additional template parameter.

IMPORTANT: In the current implementation, only the `sub_group` scope
is supported

When the group is a `sycl::sub_group`, a matrix is declared as follows:

```c++
joint_matrix<sub_group, int8_t, use::a, tM, tN, layout::row_major> tA;
```

=== Matrix Operations and their Execution Scope
We define three new functions needed to perform the main and common
operations on matrices, namely load, store, and the actual multiply
and add operation. This set of functions can be easily extended if the
matrix hardware implements new features.

Since the matrix functions are group operations (as defined in Section
4.17.3 of the SYCL specification), the matrix API has to be accessed
by all the work-items in the group in a convergent control flow. The
`Group` template argument can be a work-group or a sub-group. These
functions will be called once by each work item in the group.

To be aligned with the SYCL 2020 group algorithms, an additional group
argument is added to the matrix operations to designate that these
functions are collective operations. The {dpcpp} syntax is the following:

IMPORTANT: In the current implementation, only the `sub_group` scope
is supported.

==== Load
```c++
namespace sycl::ext::oneapi::experimental::matrix {
  template <typename Group, typename T, typename S,
            size_t NumRows, size_t NumCols,
            access::address_space Space>
  void joint_matrix_load(Group sg,
    joint_matrix<Group, T, use::accumulator, NumRows, NumCols,
    layout::dynamic> &res,
    multi_ptr<S, Space, IsDecorated> src, size_t stride, layout Layout);
    
  template <typename Group, typename T, typename S,
            size_t NumRows, size_t NumCols,
            use Use, layout Layout, access::address_space Space>
  void joint_matrix_load(Group sg,
    joint_matrix<Group, T, Use, NumRows, NumCols, Layout> &res,
    multi_ptr<S, Space, IsDecorated> src, size_t stride);
}
```

`joint_matrix_load` loads data from memory to the 2d tiles/registers
of the matrix hardware.
We define two overloads of the load function depending on whether the
memory layout was declared as part of the `joint_matrix` type or not. 
The first overload that takes memory layout as an argument is only
available for a `joint_matrix` type that used the default value
`layout::dynamic`.
The second overload without a memory layout must not be used with a
`joint_matrix` type that used the default value `layout::dynamic`.

The base pointer `src` here determines the starting address of the
matrix to be loaded from. `Layout` determines whether the data is
being read in a row (`row_major`), column major (`column_major`)
fashion. `stride` describes the number of elements between consecutive
rows for the row major layout, or between columns for the column major
layout.


==== Store
```c++
namespace sycl::ext::oneapi::experimental::matrix {
  template <typename Group, typename T, size_t NumRows, size_t NumCols,
            access::address_space Space>
  void joint_matrix_store(Group sg,
    joint_matrix<Group, T, use::accumulator, NumRows, NumCols,
    layout::dynamic> &res,
    multi_ptr<T, Space, IsDecorated> dest, size_t stride, layout Layout);
}
```
This function stores the data in the accumulator matrix from the 2d
tiles back to memory.

The base pointer `dest` here determines the starting address of the
matrix to be stored. `Layout` determines whether the data is being
written in a row (`row_major`), column major (`column_major`)
fashion. `stride` describes the number of elements between consecutive
rows for the row major layout, or between columns for the column major layout. 


==== Multiply and Add

```c++
namespace sycl::ext::oneapi::experimental::matrix {
  template <typename Group, typename Ta, typename Tb, typename Tc,
  std::size_t M, std::size_t K, std::size_t N, 
            layout LayoutA, layout LayoutB>
  joint_matrix<Group, Td, use::accumulator, M, N, layout::dynamic>
  joint_matrix_mad(Group sg,
    joint_matrix<Group, Ta, use::a, M, K, layoutA> A,
    joint_matrix<Group, Tb, use::b, K, N, layoutB> B,
    joint_matrix<Group, Tc, use::accumulator, M, N, layout::dynamic> C);
}
```
The matrix multiply and add function performs the multiply operation
on the matrices `A` and `B`, accumulates the result with `C` and returns
the result.


==== Matrix Initialization: `joint_matrix_fill`
Unlike `joint_matrix_load` that assumes that all the matrices are
directly loaded from memory, `joint_matrix_fill`  makes it possible to
multiply a matrix which is not directly loaded from memory but rather
initialized directly in the register. On Intel AMX, if the
initialization constant is zero, this would map to the `_tile_zero` intrinsic:

```c++
namespace sycl::ext::oneapi::experimental::matrix {
  template <typename Group, typename T, size_t NumRows, size_t NumCols,
           use Use, layout Layout, typename Tv>
  void joint_matrix_fill(Group sg, joint_matrix<Group, T, Use,
  NumRows, NumCols, Layout> &m, Tv v);
}
```
IMPORTANT: In the current implementation, only the `sub_group` scope
is supported.

==== Element-Wise Operations
Besides matrix multiply and add, this extension aims to make it
possible to perform piece-wise operations on matrices in a SPMD
manner. `joint_matrix_apply` function performs an element-wise
operation where the same operation is performed on every element of
the joint matrix, such that the operation can be performed without knowledge
of the position of the element within the matrix. Activation functions
or adding a constant value to every element of the matrix are two
examples of this usage. When the operation depends on the element
index of the matrix, an Intel-specific extension is available as part
of the * link:sycl_ext_intel_matrix.asciidoc[sycl_ext_intel_matrix]

Besides the `Group` and the `joint_matrix` argument,
`joint_matrix_apply` takes a lambda expression as an argument that
specifies the specific operation on each of the elements of the input
matrix.

```c++
namespace sycl::ext::oneapi::experimental::matrix {
  template<typename Group, typename T, use Use, size_t M, size_t N,
  layout Layout, typename F>
  void joint_matrix_apply(Group g, joint_matrix<Group, T, Use, M, N,
  Layout>C, F&& lambda);
}
```

In the following example, every element of the matrix `C` is
multiplied by `alpha`. Then, an activation function, `relu` in this
example, is applied on each of the elements of `C`.

```c++
joint_matrix_apply(sg, C, [=](T x) {
    x *= alpha;
    relu(x);
});
```
IMPORTANT: `joint_matrix_apply` is not implemented yet.

=== Joint Matrix Additional Types
Besides C++ `half`, `float`, `double` types, and `sycl::bfloat16` types, joint
matrix implementations may support other low-precision floating-point types
such as tf32. tf32 type has a 19 bit format with one sign bit, 8
exponent bits offering the same range as fp32,  and 10 mantissa bits
offering same precision as  half type. The usage of tf32 type is
restricted to `joint_matrix` using:
`sycl::ext::oneapi::experimental::matrix::precision::tf32`.

Joint matrix type tf32 is defined as an empty class with no member functions.
```c++
namespace precision {
  class tf32;
}
```
Besides the type, one conversion function is added:
`round_to_tf32` that  performs the rounding to tf32.

```c++
namespace sycl::ext::oneapi::experimental::matrix {
  float round_to_tf32(float &elem);
}
```
Joint matrix load/store/fill  perform float type memory access to/from
tf32 joint matrix. Also, the return type of element-wise accesses of a
tf32 `joint_matrix` returns float. In this case, general arithmetic is
done on fp32 data.


=== Example using int8_t type
```c++
using namespace sycl::ext::oneapi::experimental::matrix;

queue q;
range<2> G = {M/tM, N};
range<2> L = {1, SG_SIZE};
int8_t *memA = malloc_shared<int8_t>(M*K, q);
int8_t *memB = malloc_shared<int8_t>(K*N, q);
int32_t *memC = malloc_shared<int32_t>(M*N, q);
q.parallel_for(nd_range<2>(G, L), [=](nd_item<2> item)
  [[sycl::reqd_sub_group_size(SG_SIZE)]] {
   const auto global_idx = item.get_global_id(0);
   const auto global_idy = item.get_global_id(1);
   const auto sg_startx = global_idx - item.get_local_id(0);
   const auto sg_starty = global_idy - item.get_local_id(1);
   sub_group sg = item.get_sub_group();
   joint_matrix<sub_group, int8_t, use::a, tM, tK, layout::row_major> tA;
   joint_matrix<sub_group, int8_t, use::b, tK, tN, layout::row_major> tB;
   joint_matrix<sub_group, int32_t, use::accumulator, tM, tN> tC;
   joint_matrix_fill(sg, tC, 0);
   for (int k = 0; k < K; k += tK) {
     joint_matrix_load(sg, tA,
          multi_ptr<int8_t, sycl::access::address_space::global_space>(memA) +
          sg_startx * tM * K + k, K);
     joint_matrix_load(sg, tB,
          multi_ptr<int8_t, sycl::access::address_space::global_space>(memB) +
          k * N + sg_starty/SG_SIZE*tN, N);
     tC = joint_matrix_mad(sg, tA, tB, tC);
   }
   joint_matrix_apply(sg, tC, [=](int8_t x) {
    x *= alpha;
   });
   joint_matrix_store(sg, tC,
        multi_ptr<int32_t, sycl::access::address_space::global_space>(memC) +
	sg_startx * tM * N + sg_starty/SG_SIZE*tN, N, layout::row_major);
}).wait();
```

=== Query Interface
Intel AMX, Intel XMX and Nvidia Tensor Cores matrix hardware support different
sizes and types (see Appendix: Supported Combinations Per
Hardware). The query interface is used to validate user code and
inform them about supported types, sizes, scope, and layouts by the
implementation. This also offers development and tuning productivity
by both scientists and library developers. We provide two types of the
query interface: compile-time query and runtime query.

==== Compile-Time Query
This returns `constexpr` values to use in `joint_matrix` template
arguments but depends on an enumeration of the matrix hardware that
can be tested.  The compile-time query interface proposed here
consists of two functionalities:

- Validation: at compile time, the validation functionality informs
  the user whether a specific combination is valid or not. This takes
  place when the user specifies all template parameters.

- Default values: this provides a default shape if the user does not
  provide a specific combination. In this case, aliases to the
  `joint_matrix` type can be used, namely
  `joint_matrix_a/b/accumulator` where no additional argument is
  needed. This form happens when the user specifies all template
  parameters except the sizes of the matrices (`tiles`) M, N, and K.

The table below provides a description for each of the member
variables in `matrix_params` class and the forms in which  they are
defined.

[frame="none",options="header"]
|======================
| Member/type alias in `tpu_params` | Description
|`type_a`| type alias for the type of matrix A
|`type_b`| type alias for the type of matrix B
|`type_accumulator`| type alias for the type of matrix accumulator
|`M`|when no sizes are provided by the user, indicates the suggested
default size for M; usually this corresponds to the maximum size the
implementation supports. In validation mode, where the user does
provide sizes, this is the same value M that the user provides if M is
supported by the implementation
|`N`|when no sizes are provided by the user, indicates the suggested
default size for N; usually this corresponds to the maximum size the
implementation supports. In validation mode, where the user does
provide sizes, this is the same value N that the user provides if N is
supported by the implementation
|`K`| when no sizes are provided by the user, indicates the suggested
default size for K; usually this corresponds to the maximum size the
implementation supports. In validation mode, where the user does
provide sizes, this is the same value K that the user provides if K is
supported by the implementation
|`joint_matrix_a`| type alias for `joint_matrix` for matrix A
|`joint_matrix_b`| type alias for `joint_matrix` for matrix B |
`joint_matrix_accumulator`| type alias for `joint_matrix` for matrix
accumulator
|======================

```c++
namespace sycl::ext::oneapi::experimental::matrix {
template<tpu u, typename Ta=void, typename Tb=void, typename Tc=void,
int sM=0, int sN=0, int sK=0>
struct tpu_params;

// Validation form: Valid or not
// Specialization when both types and sizes are given
template <typename Ta, typename Tb, typename Tc, int sM, int sN, int
sK, layout>
struct tpu_params<
    tpu::amx, Ta, Tb, Tc, sM, sN, sK,
    typename std::enable_if<(
        !std::is_same_v<Ta, void> && !std::is_same_v<Tb, void> &&
        !std::is_same_v<Tc, void> && sM != 0 && sN != 0 && sK != 0)>::type> {
  // Validate that parameters are supported
  static_assert(
      (sM == 0 && sN == 0 && sK == 0) ||
          (is_combination_valid_amx<Ta, Tb, Tc>(sM, sN, sK)),
      "Invalid parameters for Intel AMX, query valid types and maximum sizes "
      "using: dev.get_info<experimental::info::device::matrix>(); and
      then check out matrix::combinations array");
  using type_a = Ta; // this type alias is not available in the
  current implementation 
  using type_b = Tb; // this type alias is not available in the
  current implementation
  using type_accumulator = Tc; // this type alias is not available in
  the current implementation

  // if combination is valid, construct the matrices

  static constexpr std::size_t M = (sM != 0) ? sM : 16;
  static constexpr std::size_t N = (sN != 0) ? sN : 16;
  static constexpr std::size_t K =
      (sK != 0) ? sK : ((sizeof(Ta) == 1) ? 64 : 32);

  template <typename Group, layout LayoutA>
  using joint_matrix_a = joint_matrix<Group, Ta, use::a, defaultM,
  defaultK, LayoutA>;
  template <typename Group, layout LayoutB>
  using joint_matrix_b = joint_matrix<Group, Tb, use::b, defaultK,
  defaultN, LayoutB>;
  template <typename Group>
  using joint_matrix_accumulator = joint_matrix<Group, Tc,
  use::accumulator, defaultM, defaultN>;
};

// Default values form: Sizes-only query
// Specialization for when only types are given, need to query only sizes
template <typename Ta, typename Tb, typename Tc>
struct tpu_params<tpu::amx, Ta, Tb, Tc, 0, 0, 0,
                  typename std::enable_if<(!std::is_same_v<Ta, void> &&
                                           !std::is_same_v<Tb, void> &&
                                           !std::is_same_v<Tc, void>)>::type> {
  static_assert((are_types_valid_amx<Ta, Tb, Tc>()),
                "Invalid types for Intel AMX, supported types are
                 int8_t, uint8_t, and bfloat16) ");

  using type_a = Ta; // this type alias is not available in the
  current implementation 
  using type_b = Tb; // this type alias is not available in the
  current implementation
  using type_accumulator = Tc; // this type alias is not available in
  the current implementation

  // construct the matrices using the default sizes
  static constexpr std::size_t M = 16;
  static constexpr std::size_t N = 16;
  static constexpr std::size_t K = ((sizeof(Ta) == 1) ? 64 : 32);

  template <typename Group, layout LayoutA>
  using joint_matrix_a = joint_matrix<Group, Ta, use::a, M, K, LayoutA>;
  template <typename Group, layout LayoutB>
  using joint_matrix_b = joint_matrix<Group, Tb, use::b, K, N, LayoutB>;
  template <typename Group>
  using joint_matrix_accumulator = joint_matrix<Group, Tc,
  use::accumulator, M, N>;
};
enum class tpu {
  xmx8,
  xmx16,
  amx
};
```
===== Validation Example:
```c++
// User can provide sizes besides the types and tpu_params can assert
  if they are supported or not
// in this case, an assertion will happens as 16 is not a supported size for M
using myparams = tpu_params<tpu::xmx16, int8_t, int8_t, int, 16, 16, 32>;
size_t NDRangeM = M / myparams::M;  //Assertion would happen at this line
size_t NDRangeN = N / myparams::N;
```

===== Default Values Example:
```c++
using myparams = tpu_params_both<tpu::xmx16, int8_t, int8_t, int>;
// use this to construct the ranges on the host side
size_t NDRangeM = M / myparams::M;
size_t NDRangeN = N / myparams::N;
//if M, N, K do not multiply the default sizes, padding has to be done
// device code: the matrices are constructed using the default dimensions
myparams::joint_matrix_a<sub_group, layout::row_major> sub_a;
myparams::joint_matrix_b<sub_group, layout::row_major> sub_b;
myparams::joint_matrix_accumulator<sub_group> sub_c;

```
==== Runtime Query
This provides a more general query interface with information about
sizes, types,  and scopes that are supported by a specific matrix
implementation. This is needed to avoid padding by the user, for
tuning, and efficient code generation if used by a library. The
general query returns an array of `combinations` of `combination`
type. Each combination includes the sizes and the types for the
matrices A, B, and accumulator. Note that for each matrix hardware,
the query returns `max_msize, max_nsize, max_ksize` or `msize, nsize,
ksize` exclusively, depending on whether the implementation supports a
continuous or discrete number of sizes. For example, the Intel AMX
implementation supports a continuous number of sizes, so the `max_*`
variant is applied and only the maximum number is returned. The Intel
XMX implementation, on the other hand, supports a discrete list of
numbers so the `msize, nsize, ksize` variant is applied.

The table below provides a description for each of the device matrix
desciptors that can be queried using `get_info` API.

[frame="none",options="header"]
|======================
| Device descriptors | Return type| Description
|`ext::oneapi::experimental::info::device::matrix::numtiles`| `uint32_t`
|indicates number of tiles in Intel AMX (does not apply to Intel XMX)
|`ext::oneapi::experimental::info::device::matrix::scopes`
|`std::vector<scope_t>`
|indicates the memory and execution scopes supported by the matrix
implementation
|`ext::oneapi::experimental::info::device::matrix::num_scopes`|`uint32_t`
|indicates number of scopes supported by the matrix implementation
|`ext::oneapi::experimental::info::device::matrix::combinations` |
`std::vector<combination>`| tells the set of supported matrix sizes
and types
|`ext::oneapi::experimental::info::device::matrix::num_combinations`|`uint32_t`
|indicates number of combinations supported by the matrix
implementation which corresponds to the size of the `combinations` array
|`combination` 
|`combination`
|composes the types and sizes of A, B, accumulator matrices allowed in
one combination
|`max_msize`, `max_nsize`, `max_ksize`| `uint32_t`| if the matrix
implementation supports a continuous number of element sizes, each of
these members is non-zero, and the matrix implementation supports all
element sizes from 1 up to (and including) that number. By contrast,
if the TPU implementation supports a discrete number of element
sizes, each of these members has the value zero
|`msize`, `nsize`, `ksize`| `uint32_t`| if the matrix implementation
supports a discrete number of element sizes, each of these members is
non-zero, and the value tells one of the supported element sizes. By
contrast, if the matrix hardware supports a continuous number of
element sizes, each of these members has the value zero
|`atype`, `btype`, `accumulatortype`| `matrix_type` | indicates the
types supported in the combination
|======================

```c++
enum class scope_t {
  sub_group,
  work_group
};
enum class matrix_type {
  bf16,
  fp16,
  tf32,
  fp32,
  fp64,
  sint2,
  sint4,
  sint8,
  sint16,
  sint32,
  sint64,
  uint2,
  uint4,
  uint8,
  uint16,
  uint32,
  uint64
};
struct combination {
  uint32_t max_msize;
  uint32_t max_nsize;
  uint32_t max_ksize;
  uint32_t msize;
  uint32_t nsize;
  uint32_t ksize;
  matrix_type atype;
  matrix_type btype;
  matrix_type accumulatortype;
};
```

===== General Query Example:
```c++
constexpr int M = 1500; // with msize = 8 and msize = 4,
          // M can be broken up to 125 sequence of 8-sized ops and
          // remaining 500 using 125 sequence of 4-sized ops
auto combinations = device.get_info<info::device::matrix::combinations>();

constexpr int {msize, nsize, ksize} = break_dimension(combinations, M);
constexpr int msize_remainder = break_dimension_remainder(combinations, M);
// device code:
joint_matrix<sub_group, int8_t, use::a, msize, ksize, layout::row_major> sub_a;
joint_matrix<sub_group, int8_t, use::b, ksize, nsize, layout::row_major> sub_b;
joint_matrix<sub_group, int, use::accumulator, msize, nsize> sub_c;
//Remainder handling
```

=== Appendix: Supported Combinations Per Hardware

The table below provides a list of the combinations that
`joint_matrix` implementations support on each of Intel AMX and Intel
XMX hardware. Note that these can be returned in a parametrized way
using the `tpu_params` query class.

==== Intel AMX Supported Combinations

[frame="none",options="header"]
|======================
| A type | B type | Accumulator type | M | N | K
| `matrix_type::(u)int8`  | `matrix_type::(u)int8` |
`matrix_type::sint32`  |  +<=+ 16 |  +<=+ 16 |  +<=+ 64
|  `matrix_type::bf16`       |  `matrix_type::bf16`   |
`matrix_type::fp32`   |  +<=+ 16 |  +<=+ 16   |  +<=+ 32
|======================

==== Intel XMX Supported Combinations

[frame="none",options="header"]
|======================
| A type | B type | Accumulator type | M | N | K
| `matrix_type::(u)int8`  | `matrix_type::(u)int8` |
`matrix_type::int32`  |  +<=+ 8 |  16 |  32
|  `matrix_type::fp16`       |  `matrix_type::fp16`   |
`matrix_type::fp32`   |  +<=+ 8 |  16   |  16
|  `matrix_type::bf16`       |  `matrix_type::bf16`   |
`matrix_type::fp32`   |  +<=+ 8 |  16   |  16
|======================


=== Revision History

[frame="none",options="header"]
|======================
|Rev |Date       |Author     |Changes
|1   |2021-04-13 |Dounia Khaldi |Initial public working draft.
|2   |2021-10-05 |Dounia Khaldi |JIT implementation on both Intel AMX and DPAS
|3   |2022-05-16 |Dounia Khaldi |Add matrix fill and piece-wise
operations support
|4   |2022-08-25 |Dounia Khaldi |Update the matrix spec by adding the
new matrix use parameter and remove reference to the AOT AMX initial
implementation 
|5   |2022-11-07 |Dounia Khaldi |Update the matrix spec by making it
portable across Intel AMX, Intel XMX and Nvidia Tensor Cores, and move
the Intel-specifics to a separate extension document.
|6   |2023-01-09 |Dounia Khaldi |Add `joint_matrix_apply` API, tf32
type, runtime query, and supported combinations appendix.
|======================
