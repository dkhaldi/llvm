= sycl_ext_intel_matrix

:source-highlighter: coderay
:coderay-linenums-mode: table

// This section needs to be after the document title.
:doctype: book
:toc2:
:toc: left
:encoding: utf-8
:lang: en
:dpcpp: pass:[DPC++]

// Set the default source code type in this document to C++,
// for syntax highlighting purposes.  This is needed because
// docbook uses c++ and html5 uses cpp.
:language: {basebackend@docbook:c++:cpp}


== Notice

Copyright (c) 2022-2022 Intel Corporation.  All rights reserved.

NOTE: Khronos(R) is a registered trademark and SYCL(TM) and SPIR(TM) are
trademarks of The Khronos Group Inc.  OpenCL(TM) is a trademark of Apple Inc.
used by permission by Khronos.

== Contact

To report problems with this extension, please open a new issue at:

https://github.com/intel/llvm/issues

== Dependencies

This extension is written against the SYCL 2020 revision 6 specification.  All
references below to the "core SYCL specification" or to section numbers in the
SYCL specification refer to that revision.

This extension also depends on the following other SYCL extensions:

* link:../experimental/sycl_ext_oneapi_matrix/sycl_ext_oneapi_matrix.asciidoc[
  sycl_ext_oneapi_matrix]

== Status
This is an experimental extension specification, intended to provide early
access to features and gather community feedback.  Interfaces defined in this
specification are implemented in {dpcpp}, but they are not finalized and may
change incompatibly in future versions of {dpcpp} without prior notice.
*Shipping software products should not rely on APIs defined in this
specification.*

== Backend support status
This document describes the extra features and details for the
implementation of `joint_matrix` extension on Intel AMX and Intel
XMX.

== Overview
The Intel backend implementations on both Intel AMX and Intel XMX
support `joint_matrix`, `joint_matrix_load`, `joint_matrix_store`,
`joint_matrix_mad`, `joint_matrix_fill`, `joint_matrix_apply`, and the
query interface, as they are defined in the sycl_ext_oneapi_matrix
extension. Besides element-wise operations with mapping information,
there are additional specifics about the supported layouts that enable
extra performance and functionality listed in this document.
This extension presents some supplementary Intel AMX and Intel XMX
features not contained within the sycl_ext_oneapi_matrix
extension. The additional features are built on top of the
sycl_ext_oneapi_matrix extension but are only supported by the Intel
AMX and Intel XMX backends.

== Specification

=== Feature test macro

This extension provides a feature-test macro as described in the core SYCL
specification. An implementation supporting this extension must
predefine the macro `SYCL_EXT_INTEL_MATRIX` to one of the values
defined in the table below.Applications can test for the existence of
this macro to determine if the implementation supports this feature,
or applications can test the macro's value to determine which of the
extension's APIs the implementation supports.

[%header,cols="1,5"]
|===
|Value
|Description

|1
|The APIs of this experimental extension are not versioned, so the
 feature-test macro always has this value.
|===


=== Joint Matrix Intel-Specific Matrix Features

==== Layout
Besides row major and column major layouts, `layout` introduces the
custom layout packed layout that refers to the VNNI format descibed in
the following section.

```c++
namespace sycl::ext::intel::experimental::matrix {

enum class layout {
  packed
};

} // namespace sycl::ext::intel::experimental::matrix
```


==== Layout argument in `joint_matrix_load`
`layout` in `joint_matrix_load` can take `packed` as argument to
specify that the data has already been transformed into VNNI format
(`packed`). in this case, `stride` argument of `joint_matrix_load`
describes the number of elements between consecutive rows for packed
layouts.

In order to get maximum performance on Intel AMX and Intel XMX,
prepacking data in the memory is necessary. If users did not specify
the packed layouts, transforms done by the implementation will be slow
due to extra scatter/gather operations. Hence, we expose the `packed`
layout to the user to specify that A or B have already been
VNNIed. The packed or VNNI layout is introduced in the `VNNI layout`
section below.

IMPORTANT: In the current Intel AMX and Intel XMX implementations, the
layout in the load of matrix B (provided by the `layout memL`
parameter below) must be `packed` or `row_major`. Automatic VNNI
transform is supported on AMX. The layout in the load of matrices A
and C must be `row_major`, and the layout in the store of matrix C
(provided by the `layout memL` parameter below) must also be
`row_major`.

==== Store Operation
Besides store of matrix `accumulator`, the Intel implementation allows
store on matrix `a` and `b` as well.

```c++
namespace sycl::ext::intel::experimental::matrix {

template <typename Group, typename T, typename S,
          size_t NumRows, size_t NumCols,
          use Use, layout Layout,
          access::address_space Space, access::decorated IsDecorated>
  void joint_matrix_store(Group g,
    joint_matrix<Group, T, Use, NumRows, NumCols, Layout> &res,
    multi_ptr<S, Space, IsDecorated> src, size_t stride);

} // namespace sycl::ext::intel::experimental::matrix
```

==== Element Indexing and Piece-Wise Operations
===== Background
Besides matrix multiply and add, this extension aims to make it
possible to perform piece-wise operations on matrices in a SPMD
manner. The mechanisms that are recommended to perform such piece-wise
operations depend upon which of the following classes the operation
falls into:

Class 1- Element-wise operations where the same operation is performed
on every element of the matrix, such that the operation can be
performed without knowledge of the position of the element within the
matrix. Activation functions or adding a constant value to every
element of the matrix are two examples. In this case
`joint_matrix_apply` should be used. 

Class 2- Piece-wise operations where the operation depends on the
element index of the matrix or the operation takes multiple elements
as operands (such as a sum of all elements in a row for
example). Quantization that is needed for conversion between low
precision types like `int8_t` and `fp32` uses piece-wise operations.

// We explored multiple options to enable this feature in the matrix
interface: 1) Allowing non-restrictive element indexing on the matrix
elements would result into slow indexing on the GPU, 2) Operator
overloading can represent only element-wise operations and not the
operations on pieces (row, column, diagonal, etc) of the matrix. 3)
Providing specific functions for these piece-wise operations can
resolve some of the functions we know of today like the ones involved
in quantization but it is not general to any problem that may occur in
the future. 

===== Explicit conversion with mapping from SIMD to SPMD
The data elements in a `joint_matrix` are distributed or shared across
the work-items in the Group in an implementation-defined way. There is
no fixed allocation of matrix elements owned by a `joint_matrix`
instance to the WIs comprising the group used to instantiate it. For
instance, the matrix is a shared entity among the work items in the
case of the AMX backend because the AMX tile that holds the matrix
data is a 2d register that is shared among the work items. Therefore
the partitioning among the WIs is implementation defined. However, it
is necessary to allocate WIs to specific elements of the matrix in
order to perform element-wise operations. In order to be able to
perform element-wise operations in a general and efficient way, we
provide a conversion function from the `joint_matrix` domain that is
owned by a group of work items to the portion that is owned by each
work item. This enables the WI to perform piece-wise operations on the
matrix within the SYCL SPMD programming model.

We introduce a new function `get_wi_data` that provides a view of the
portion of the matrix that is owned by the current WI. The indexing
provided inside the `wi_data` class accesses only the portion of the
current WI and returns  `wi_element`. This latter holds a reference to
the original joint_matrix that `wi_data` was constructed from. This
means that modifying `wi_data` also modifies the corresponding joint
matrix elements. Users can use the `=` operator to update the element
of the `joint_matrix` represented by the `wi_element` after the
element-wise operation.

Using `get_wi_data`, it is not possible to know which portions of data
are owned by each thread in the group as this is implementation
defined and changes from one backend to the other. For general
piece-wise operations such as summing the rows of a matrix, the WI
data to joint matrix mapping coordinates information must be known in
order to reason about the matrix view and extract the relevant
piece. However, for element-wise operations where the same operation
is performed on all the elements of the matrix, having all the WIs in
the group apply the operation inside a loop iterating over the
`length` of `wi_data` guarantees the whole matrix element-wise operation.

Note that `get_wi_data` cannot return a fixed size array length
because the length of the WI portion is a runtime variable for the
following reasons:

1- The main compilation mode of SYCL is JIT compilation and
partitioning among WIs is implementation defined.

2- Sub group size is not generally fixed.

The code listing below shows a synopsis of these new APIs.

```c++
namespace sycl::ext::intel::experimental::matrix {

wi_data<group, T, Use, Rows, Cols, Layout> get_wi_data(Group g,
 joint_matrix<Group, T, Use, Rows, Cols, Layout> Mat);

template <typename T, size_t Rows, size_t Cols, use Use, layout
Layout, typename Group>
class wi_data {
  size_t length();
  wi_element<T, NumRows, NumCols, Use, Layout, Group> operator[](size_t i);
};
template <typename T, size_t Rows, size_t Cols,
          use Use, layout Layout,
          typename Group = sycl::sub_group>
class wi_element {
  operator T();
  wi_element &operator=(const T &rhs);
  wi_element &operator+=(const T &rhs);
  wi_element &operator-=(const T &rhs);
  wi_element &operator*=(const T &rhs);
  wi_element &operator/=(const T &rhs);

  std::tuple<size_t, size_t> get_coord();
};

} // namespace sycl::ext::intel::experimental::matrix
```

In the following example `wi_data_c` is a reference to the WI owned
portion of the joint matrix `matC`. As such `wi_data_c[i] OP rhs`
updates the corresponding matrix element in the joint_matrix `matC`.
Vectorization along the sub group dimension will get enabled
automatically to vectorize the contiguous portion of the matrix.


```c++
auto wi_data_c = get_wi_data(sg, matC);
for (int i = 0; i < wi_data_c.length(); i++)
        wi_data_c[i] *= alpha;    // Note that the indexing here "i"
	is in the vector owned by a WI, not in the matrix C
```

IMPORTANT: In the current implementation, only the `sub_group` scope
is supported.

===== Work-item data to joint matrix mapping coordinates
The `wi_data` and `wi_element` classes provide access to the matrix
elements that are local to the calling work-item. However, the
distribution of matrix elements to each work-item is
implementation-defined, so application code cannot assume any fixed
distribution. Instead, application code can use the `get_coord` method
to query the matrix coordinates of an individual `wi_element`.

`get_coord` returns [row,col] coordinates of the current object
`wi_element` of the joint matrix.  The code above results into the following:

```c++
auto data = get_wi_data(sg, tA);
// each WI calculates local sum of rows
for (int i = 0; i < data.length(); ++i) {
  auto [row, col] = data[i].get_coord();
  sum_of_local_rows[row] += data[i];
}
```

IMPORTANT: `get_coord` is not implemented yet.

==== VNNI/Packed Layout
Intel AMX and Intel XMX compute assumes that the B tile register
(src1) is in the VNNI format as they need 32bit of K-data in A and B
to be contiguous in memory.
The VNNI blocking factor is 2 in the case of 16-bit types, and it is 4
in the case of 8-bit types. While the current implementation assumes
that the matrix has been already packed by the user for performance
reasons, the layout information is needed to inform the implementation
about this transformation.  The following example illustrates how a
matrix in `row_major` layout is transformed into the `packed` layout
for a 16-bit type.

===== Example 1: 16-bit elements
      // Example of a 4 row x 4 column matrix using a 16-bit data
      element, in row-major layout.
      // Element a1 is contiguous in memory with element b1, etc.
      // ---------------------------------
      // a1, b1, c1, d1
      // a2, b2, c2, d2
      // a3, b3, c3, d3
      // a4, b4, c4, d4
      // ---------------------------------
      // The same matrix reformatted in packed layout.
      // Here, packing of 2 elements is needed to form 32 bits.
      // Element a1 is contiguous in memory with element a2, etc.
      // ---------------------------------
      // a1, a2, b1, b2, c1, c2, d1, d2
      // a3, a4, b3, b4, c3, c4, d3, d4

===== Example 2: 8-bit elements

      // Example of a 4 row x 4 column matrix using a 8-bit data
      element, in row-major layout.
      // Element a1 is contiguous in memory with element b1, etc.
      // ---------------------------------
      // a1, b1, c1, d1
      // a2, b2, c2, d2
      // a3, b3, c3, d3
      // a4, b4, c4, d4
      // ---------------------------------
      // The same matrix reformatted in packed layout.
      // Here, packing of 4 elements is needed to form 32 bits.
      // Elements a1, a2, a3, a4 are contiguous in memory, etc.
      // ---------------------------------
      // a1, a2, a3, a4, b1, b2, b3, b4, c1, c2, c3, c4, d1, d2, d3, d4

=== Example using int8_t type
```c++
using namespace sycl::ext::oneapi::experimental::matrix;

queue q;
range<2> G = {M/tM, N};
range<2> L = {1, SG_SIZE};
int8_t *memA = malloc_shared<int8_t>(M*K, q);
int8_t *memB = malloc_shared<int8_t>(K*N, q);
int32_t *memC = malloc_shared<int32_t>(M*N, q);
q.parallel_for(nd_range<2>(G, L), [=](nd_item<2> item)
  [[sycl::reqd_sub_group_size(SG_SIZE)]] {
   const auto global_idx = item.get_global_id(0);
   const auto global_idy = item.get_global_id(1);
   const auto sg_startx = global_idx - item.get_local_id(0);
   const auto sg_starty = global_idy - item.get_local_id(1);
   sub_group sg = item.get_sub_group();
   joint_matrix<sub_group, int8_t, use::a, tM, tK, layout::row_major> tA;
   joint_matrix<sub_group, int8_t, use::b, tK, tN,
                ext::intel::experimental::matrix::layout::packed> tB;
   joint_matrix<sub_group, int32_t, use::accumulator, tM, tN> tC;
   joint_matrix_fill(sg, tC, 0);
   for (int k = 0; k < K; k += tK) {
     joint_matrix_load(sg, tA,
          multi_ptr<int8_t, sycl::access::address_space::global_space>(memA) +
	  sg_startx * tM * K + k, K);
     joint_matrix_load(sg, tB,
          multi_ptr<int8_t, sycl::access::address_space::global_space>(memB) +
	  k * N*4 + sg_starty/SG_SIZE*tN*4, N*4);
     tC = joint_matrix_mad(sg, tA, tB, tC);
   }
   auto wi_data_c = ext::intel::experimental::matrix::get_wi_data(sg, tC);
   for (int i = 0; i < wi_data_c.length(); i++)
     wi_data_c[i] *= alpha;
   joint_matrix_store(sg, tC,
        multi_ptr<int32_t, sycl::access::address_space::global_space>(memC) +
	sg_startx * tM * N + sg_starty/SG_SIZE*tN, N, layout::row_major);
}).wait();
```

=== Intel-Specific Runtime Query
Besides the query we provide in
../experimental/sycl_ext_oneapi_matrix/sycl_ext_oneapi_matrix.asciidoc[sycl_ext_oneapi_matrix],
some device descriptors are Intel hardware specific. These are
provided as part of `ext::intel::experimental::info::device::matrix`
namespace:

[frame="none",options="header"]
|======================
| Device descriptors | Return type| Description
|`ext::oneapi::experimental::info::device::matrix::numtiles`| `uint32_t`
|indicates number of tiles in Intel AMX (does not apply to Intel XMX)
|======================

== Revision History

[frame="none",options="header"]
|======================
|Rev |Date       |Author     |Changes
|1   |2022-11-07 |Dounia Khaldi |Add Intel-specific store API,
layout information, iterative-based element-wise operations, and
mapping 
|======================
